{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 9028970,
     "sourceType": "datasetVersion",
     "datasetId": 5441790
    }
   ],
   "dockerImageVersionId": 30746,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Beijing Air Quality Data Analysis and Application**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  **1.Data Preprocessing and Exploratory Data Analysis (EDA)**\n",
    "\n",
    "**Objective:** Clean the data, understand the data distribution and characteristics, and identify potential issues and data trends.\n",
    "\n",
    "**Methods Used:**\n",
    "\n",
    "\t•\tData Cleaning: Handle missing values, duplicates, and outliers.\n",
    "\t•\tpandas: dropna(), fillna(), drop_duplicates()\n",
    "\t•\tnumpy: Data processing and manipulation\n",
    "\t•\tData Visualization: Plot time series, box plots, histograms for various pollutants and meteorological parameters.\n",
    "\t•\tmatplotlib, seaborn"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# read all tables\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for i in range(0, 12):\n",
    "    df = pd.concat([df, pd.read_csv(f\"/kaggle/input/beijingclimatedata/PRSA_Data_{i}.csv\")], axis = 0)\n",
    "\n",
    "data = df.copy()\n",
    "\n",
    "print(\"There are {} missing values in this dataset\".format(data.isnull().sum().sum()))\n",
    "print('Number of instances = %d' % (data.shape[0]))\n",
    "print('Number of attributes = %d' % (data.shape[1]))\n",
    "\n",
    "# Count the number of rows with missing values\n",
    "rows_with_missing = data.isnull().any(axis=1).sum()\n",
    "print(f\"Rows with missing values: {rows_with_missing}\")\n",
    "\n",
    "# Count the number of columns with missing values\n",
    "columns_with_missing = data.isnull().any(axis=0).sum()\n",
    "print(f\"Columns with missing values: {columns_with_missing}\")\n",
    "\n",
    "print('\\nNumber of missing values:')\n",
    "for col in data.columns:\n",
    "    print('\\t%s: %d' % (col,data[col].isna().sum()))\n",
    "\n",
    "data.columns"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-07-25T10:06:03.966291Z",
     "iopub.execute_input": "2024-07-25T10:06:03.966716Z",
     "iopub.status.idle": "2024-07-25T10:06:06.533403Z",
     "shell.execute_reply.started": "2024-07-25T10:06:03.966674Z",
     "shell.execute_reply": "2024-07-25T10:06:06.532261Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# remove the empty data rows"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# remove the empty data rows\n",
    "data = data.dropna()\n",
    "\n",
    "# Count the number of rows with missing values\n",
    "rows_with_missing = data.isnull().any(axis=1).sum()\n",
    "print(f\"Rows with missing values: {rows_with_missing}\")\n",
    "\n",
    "# Count the number of columns with missing values\n",
    "columns_with_missing = data.isnull().any(axis=0).sum()\n",
    "print(f\"Columns with missing values: {columns_with_missing}\")\n",
    "\n",
    "print('Number of instances = %d' % (data.shape[0]))\n",
    "\n",
    "# data.describe()\n",
    "# data.head()\n",
    "data.dtypes.value_counts()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T10:06:09.592068Z",
     "iopub.execute_input": "2024-07-25T10:06:09.592559Z",
     "iopub.status.idle": "2024-07-25T10:06:09.793351Z",
     "shell.execute_reply.started": "2024-07-25T10:06:09.592518Z",
     "shell.execute_reply": "2024-07-25T10:06:09.792239Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convert Time and Wind Direction Column"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert Time Columns\n",
    "data['datetime'] = pd.to_datetime(data[['year', 'month', 'day', 'hour']])\n",
    "data.set_index('datetime', inplace=True)\n",
    "\n",
    "# Convert Wind Direction Column\n",
    "direction_map = {\n",
    "    'N': 0, 'NNE': 22.5, 'NE': 45, 'ENE': 67.5,\n",
    "    'E': 90, 'ESE': 112.5, 'SE': 135, 'SSE': 157.5,\n",
    "    'S': 180, 'SSW': 202.5, 'SW': 225, 'WSW': 247.5,\n",
    "    'W': 270, 'WNW': 292.5, 'NW': 315, 'NNW': 337.5\n",
    "}\n",
    "\n",
    "data['wd_numeric'] = data['wd'].map(direction_map)\n",
    "\n",
    "# Remove the string column, there are restrictions when taking the mean later.\n",
    "data = data.drop(columns = ['wd', 'station'], axis=1)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T07:55:51.673983Z",
     "iopub.execute_input": "2024-07-25T07:55:51.674345Z",
     "iopub.status.idle": "2024-07-25T07:55:52.331927Z",
     "shell.execute_reply.started": "2024-07-25T07:55:51.67431Z",
     "shell.execute_reply": "2024-07-25T07:55:52.330806Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plot Time Series"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# downsample the data，take one data point per day\n",
    "df_daily = data.resample('D').mean()\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(df_daily['PM2.5'], label='PM2.5')\n",
    "plt.plot(df_daily['PM10'], label='PM10')\n",
    "plt.plot(df_daily['SO2'], label='SO2')\n",
    "plt.plot(df_daily['NO2'], label='NO2')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Concentration (µg/m³)')\n",
    "plt.title('Daily Mean Time Series of Air Pollutants')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T03:48:51.566894Z",
     "iopub.execute_input": "2024-07-25T03:48:51.567311Z",
     "iopub.status.idle": "2024-07-25T03:48:52.241295Z",
     "shell.execute_reply.started": "2024-07-25T03:48:51.567274Z",
     "shell.execute_reply": "2024-07-25T03:48:52.240158Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plot meteorological parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "plt.plot(df_daily['TEMP'], label='Temperature (°C)')\n",
    "plt.plot(df_daily['RAIN'], label='Rainfall (mm)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Daily Mean Time Series of Meteorological Parameters')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T03:40:51.012862Z",
     "iopub.execute_input": "2024-07-25T03:40:51.013738Z",
     "iopub.status.idle": "2024-07-25T03:40:51.467876Z",
     "shell.execute_reply.started": "2024-07-25T03:40:51.01369Z",
     "shell.execute_reply": "2024-07-25T03:40:51.466694Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plot Histograms"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# sample 10% of the data before plotting histograms\n",
    "df_sample = data.sample(frac=0.1, random_state=1)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "df_sample[['PM2.5', 'PM10', 'SO2', 'NO2']].hist(bins=30, layout=(2, 2), figsize=(14, 8))\n",
    "plt.suptitle('Histogram of Air Pollutants')\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T03:51:42.964574Z",
     "iopub.execute_input": "2024-07-25T03:51:42.965023Z",
     "iopub.status.idle": "2024-07-25T03:51:44.023992Z",
     "shell.execute_reply.started": "2024-07-25T03:51:42.964988Z",
     "shell.execute_reply": "2024-07-25T03:51:44.022833Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Histograms for meteorological parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "df_sample[['TEMP', 'RAIN']].hist(bins=30, layout=(1, 2), figsize=(14, 8))\n",
    "plt.suptitle('Histogram of Meteorological Parameters')\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T03:51:49.113641Z",
     "iopub.execute_input": "2024-07-25T03:51:49.114075Z",
     "iopub.status.idle": "2024-07-25T03:51:49.764438Z",
     "shell.execute_reply.started": "2024-07-25T03:51:49.114039Z",
     "shell.execute_reply": "2024-07-25T03:51:49.763311Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plot Joint Distribution of Wind Speed and Wind Direction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# a scatter plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.scatterplot(x='wd_numeric', y='WSPM', data=df_sample)\n",
    "plt.title('Wind Speed vs Wind Direction')\n",
    "plt.xlabel('Wind Direction (Degrees)')\n",
    "plt.ylabel('Wind Speed (m/s)')\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T03:51:55.805747Z",
     "iopub.execute_input": "2024-07-25T03:51:55.806587Z",
     "iopub.status.idle": "2024-07-25T03:51:56.269087Z",
     "shell.execute_reply.started": "2024-07-25T03:51:55.806547Z",
     "shell.execute_reply": "2024-07-25T03:51:56.26812Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **2. Time Series Analysis**\n",
    "**Objective:** Analyze the trends and seasonal patterns of air pollutants over time.\n",
    "\n",
    "**Methods Used:**\n",
    "\n",
    "* Time Series Decomposition: Decompose into trend, seasonal, and residual components.\n",
    "* statsmodels: seasonal_decompose\n",
    "* ARIMA Model: Forecast future air pollutant concentrations.\n",
    "* statsmodels: ARIMA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Time Series Decomposition: Decompose into trend, seasonal, and residual components."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "\n",
    "# data.columns\n",
    "\n",
    "data['datetime'] = pd.to_datetime(data[['year', 'month', 'day', 'hour']])\n",
    "data.set_index('datetime', inplace=True)\n",
    "\n",
    "# Downsample to daily data\n",
    "df_daily = data.resample('D').mean()\n",
    "\n",
    "# time series decomposition\n",
    "decomposition = seasonal_decompose(df_daily['PM2.5'], model='additive')\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "# Plot decomposition results\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(411)\n",
    "plt.plot(df_daily['PM2.5'], label='Original')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label='Trend')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal, label='Seasonality')\n",
    "plt.legend(loc='best')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label='Residuals')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T05:54:16.960549Z",
     "iopub.execute_input": "2024-07-25T05:54:16.961581Z",
     "iopub.status.idle": "2024-07-25T05:54:18.900194Z",
     "shell.execute_reply.started": "2024-07-25T05:54:16.961539Z",
     "shell.execute_reply": "2024-07-25T05:54:18.898911Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ARIMA Model: Forecast future air pollutant concentrations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Split training set and test set\n",
    "train = df_daily['PM2.5'][:int(0.8*len(df_daily))]\n",
    "test = df_daily['PM2.5'][int(0.8*len(df_daily)):]\n",
    "\n",
    "# Fit ARIMA model\n",
    "arima_model = ARIMA(train, order=(5,1,0))\n",
    "arima_result = arima_model.fit()\n",
    "\n",
    "# predict\n",
    "arima_forecast = arima_result.forecast(steps=len(test))\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train, label='Train')\n",
    "plt.plot(test, label='Test')\n",
    "plt.plot(arima_forecast, label='ARIMA Forecast')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T05:55:46.680695Z",
     "iopub.execute_input": "2024-07-25T05:55:46.681164Z",
     "iopub.status.idle": "2024-07-25T05:55:48.5188Z",
     "shell.execute_reply.started": "2024-07-25T05:55:46.681126Z",
     "shell.execute_reply": "2024-07-25T05:55:48.517668Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **3. Clustering Analysis**\n",
    "**Objective:** Classify air quality in different districts or time periods, identifying similar regions or periods.\n",
    "\n",
    "**Methods Used:**\n",
    "\n",
    "* K-means Clustering: Cluster air quality data from different districts. \n",
    "* scikit-learn: KMeans \n",
    "* Hierarchical Clustering: Cluster air quality data from different time periods. \n",
    "* scikit-learn: AgglomerativeClustering"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# use the elbow method to figure out how many clusters we have"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn import cluster\n",
    "\n",
    "# Select features for clustering\n",
    "features = ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3']\n",
    "X = data[features].dropna()\n",
    "\n",
    "numClusters = [1,2,3,4,5,6,7,8,9,10]\n",
    "SSE = []\n",
    "for k in numClusters:\n",
    "    k_means = cluster.KMeans(n_clusters=k)\n",
    "    k_means.fit(X)\n",
    "    SSE.append(k_means.inertia_)\n",
    "plt.plot(numClusters, SSE)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('SSE')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T04:59:16.306735Z",
     "iopub.execute_input": "2024-07-25T04:59:16.307186Z",
     "iopub.status.idle": "2024-07-25T04:59:42.281035Z",
     "shell.execute_reply.started": "2024-07-25T04:59:16.307152Z",
     "shell.execute_reply": "2024-07-25T04:59:42.279898Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# use the silhouette analysis technique to determe the optimal value of k"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "# Select features for clustering\n",
    "features = ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3']\n",
    "X = df_daily[features]\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "    \n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "    \n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "    \n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\n",
    "        \"For n_clusters =\", \n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "    \n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "    \n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "        \n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        \n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        \n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "        \n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        \n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10 # 10 for the 0 samples\n",
    "        \n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "    \n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    \n",
    "    ax1.set_yticks([]) # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    \n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X.iloc[:, 0], X.iloc[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\")\n",
    "    \n",
    "    # Labelling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    \n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(\n",
    "        centers[:, 0],\n",
    "        centers[:, 1],\n",
    "        marker=\"o\",\n",
    "        c=\"white\",\n",
    "        alpha=1,\n",
    "        s=200,\n",
    "        edgecolor=\"k\",\n",
    "    )\n",
    "    \n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
    "        \n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Latitude\")\n",
    "    ax2.set_ylabel(\"Longitude\")\n",
    "    \n",
    "    plt.suptitle(\n",
    "        \"Silhouette analysis for KMeans clustering on sample data with n_clusters= %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T05:11:16.775681Z",
     "iopub.execute_input": "2024-07-25T05:11:16.776117Z",
     "iopub.status.idle": "2024-07-25T05:11:20.788808Z",
     "shell.execute_reply.started": "2024-07-25T05:11:16.776082Z",
     "shell.execute_reply": "2024-07-25T05:11:20.787429Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "\n",
    "# Select features for clustering\n",
    "features = ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3']\n",
    "X = data[features].dropna()\n",
    "\n",
    "# Perform K-means clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "kmeans.fit(X)\n",
    "data['kmeans_cluster'] = kmeans.labels_\n",
    "\n",
    "# Visualize clustering results\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.scatterplot(x='PM2.5', y='PM10', hue='kmeans_cluster', data=data, palette='viridis')\n",
    "plt.title('K-means Clustering of Air Quality Data')\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T05:16:01.722935Z",
     "iopub.execute_input": "2024-07-25T05:16:01.723581Z",
     "iopub.status.idle": "2024-07-25T05:16:22.665868Z",
     "shell.execute_reply.started": "2024-07-25T05:16:01.723539Z",
     "shell.execute_reply": "2024-07-25T05:16:22.664641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Perform cluster analysis on different time periods\n",
    "# Take the daily average\n",
    "df_daily = data.resample('D').mean().dropna()\n",
    "\n",
    "# Select features for clustering\n",
    "X_daily = df_daily[features]\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "hc = AgglomerativeClustering(n_clusters=2)\n",
    "hc_labels = hc.fit_predict(X_daily)\n",
    "df_daily['hc_cluster'] = hc_labels\n",
    "\n",
    "# Visualize clustering results\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.scatterplot(x='PM2.5', y='PM10', hue='hc_cluster', data=df_daily, palette='viridis')\n",
    "plt.title('Hierarchical Clustering of Daily Mean Air Quality Data')\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T07:02:46.610696Z",
     "iopub.execute_input": "2024-07-25T07:02:46.61115Z",
     "iopub.status.idle": "2024-07-25T07:02:47.100149Z",
     "shell.execute_reply.started": "2024-07-25T07:02:46.611112Z",
     "shell.execute_reply": "2024-07-25T07:02:47.097526Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Standardize features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform PCA dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Perform K-means clustering\n",
    "kmeans_pca = KMeans(n_clusters=2, random_state=0)\n",
    "kmeans_pca.fit(X_pca)\n",
    "data['kmeans_pca_cluster'] = kmeans_pca.labels_\n",
    "\n",
    "# Visualize clustering results\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=data['kmeans_pca_cluster'], palette='viridis')\n",
    "plt.title('K-means Clustering with PCA')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T05:16:55.501345Z",
     "iopub.execute_input": "2024-07-25T05:16:55.502477Z",
     "iopub.status.idle": "2024-07-25T05:17:12.526023Z",
     "shell.execute_reply.started": "2024-07-25T05:16:55.502432Z",
     "shell.execute_reply": "2024-07-25T05:17:12.524695Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **4. Association Rule Mining**\n",
    "**Objective:** Discover association patterns between air pollutants and between air pollutants and meteorological parameters.\n",
    "\n",
    "**Methods Used:** \n",
    "\n",
    "* Apriori Algorithm: Mine frequent itemsets and association rules.\n",
    "* mlxtend: apriori, association_rules"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 转换时间列\n",
    "data['datetime'] = pd.to_datetime(data[['year', 'month', 'day', 'hour']])\n",
    "data.set_index('datetime', inplace=True)\n",
    "\n",
    "\n",
    "# 数据离散化\n",
    "\n",
    "# 选择需要离散化的列\n",
    "columns_to_discretize = ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN']\n",
    "\n",
    "# 使用pd.qcut进行离散化\n",
    "for col in columns_to_discretize:\n",
    "    data[col + '_bin'] = pd.qcut(data[col], q=2, labels=False, duplicates='drop')\n",
    "\n",
    "# 选择离散化后的列\n",
    "discretized_columns = [col + '_bin' for col in columns_to_discretize]\n",
    "\n",
    "# 转换为one-hot编码\n",
    "df_onehot = pd.get_dummies(data[discretized_columns])\n",
    "\n",
    "# 检查转换结果\n",
    "print(df_onehot.head())\n",
    "\n",
    "# 挖掘频繁项集\n",
    "frequent_itemsets = apriori(df_onehot, min_support=0.05, use_colnames=True)\n",
    "\n",
    "# 挖掘关联规则\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.0)\n",
    "\n",
    "# 显示关联规则\n",
    "# print(rules)\n",
    "\n",
    "# 选择提升度最大的前10条规则进行可视化\n",
    "top_rules = rules.sort_values('lift', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(top_rules)), top_rules['lift'], align='center')\n",
    "plt.yticks(range(len(top_rules)), top_rules['antecedents'].astype(str) + ' -> ' + top_rules['consequents'].astype(str))\n",
    "plt.xlabel('Lift')\n",
    "plt.title('Top 10 Association Rules by Lift')\n",
    "plt.show()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T08:21:32.050259Z",
     "iopub.execute_input": "2024-07-25T08:21:32.050688Z",
     "iopub.status.idle": "2024-07-25T08:21:35.005938Z",
     "shell.execute_reply.started": "2024-07-25T08:21:32.050654Z",
     "shell.execute_reply": "2024-07-25T08:21:35.004977Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **5. Classification Analysis**\n",
    "**Objective:** Predict air quality levels (e.g., good, moderate, lightly polluted) for a specific district based on historical data.\n",
    "\n",
    "**Methods Used:**\n",
    "\n",
    "* Decision Tree: Build a classification model for air quality levels based on various pollutants and meteorological parameters.\n",
    "* scikit-learn: DecisionTreeClassifier\n",
    "* Support Vector Machine (SVM): Classify air quality levels.\n",
    "* scikit-learn: SVC\n",
    "* Random Forest: Enhance the accuracy and stability of the classification model.\n",
    "* scikit-learn: RandomForestClassifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 转换时间列\n",
    "data['datetime'] = pd.to_datetime(data[['year', 'month', 'day', 'hour']])\n",
    "data.set_index('datetime', inplace=True)\n",
    "\n",
    "#定义空气质量等级\n",
    "def categorize_pm25(pm25):\n",
    "    if pm25 <= 50:\n",
    "        return 'Excellent'\n",
    "    elif pm25 <= 100:\n",
    "        return 'Good'\n",
    "    elif pm25 <= 150:\n",
    "        return 'Light Pollution'\n",
    "    elif pm25 <= 200:\n",
    "        return 'Moderate Pollution'\n",
    "    elif pm25 <= 300:\n",
    "        return 'Heavy Pollution'\n",
    "    else:\n",
    "        return 'Severe Pollution'\n",
    "\n",
    "data['AQI_Category'] = data['PM2.5'].apply(categorize_pm25)\n",
    "\n",
    "# 特征选择和数据分割\n",
    "features = ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'WSPM']\n",
    "X = data[features]\n",
    "y = data['AQI_Category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 对特征进行标准化处理\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T10:06:19.138884Z",
     "iopub.execute_input": "2024-07-25T10:06:19.139369Z",
     "iopub.status.idle": "2024-07-25T10:06:20.709862Z",
     "shell.execute_reply.started": "2024-07-25T10:06:19.139335Z",
     "shell.execute_reply": "2024-07-25T10:06:20.708731Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# 构建和评估分类模型\n",
    "# 决策树\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "dt_classifier.fit(X_train_scaled, y_train)\n",
    "y_pred_dt = dt_classifier.predict(X_test_scaled)\n",
    "\n",
    "print(\"Decision Tree Classification Report\")\n",
    "print(classification_report(y_test, y_pred_dt))\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T10:06:25.617463Z",
     "iopub.execute_input": "2024-07-25T10:06:25.617885Z",
     "iopub.status.idle": "2024-07-25T10:06:29.541009Z",
     "shell.execute_reply.started": "2024-07-25T10:06:25.617845Z",
     "shell.execute_reply": "2024-07-25T10:06:29.539863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# 支持向量机（SVM）\n",
    "# svm_classifier = SVC(random_state=42)\n",
    "# svm_classifier.fit(X_train_scaled, y_train)\n",
    "# y_pred_svm = svm_classifier.predict(X_test_scaled)\n",
    "\n",
    "# print(\"SVM Classification Report\")\n",
    "# print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # Apply PCA\n",
    "# pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
    "# X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "# X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# svm_classifier = SVC(random_state=42)\n",
    "# svm_classifier.fit(X_train_pca, y_train)\n",
    "# y_pred_svm = svm_classifier.predict(X_test_pca)\n",
    "\n",
    "# print(\"SVM (PCA) Classification Report\")\n",
    "# print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Using linear kernel\n",
    "svm_classifier = SVC(kernel='linear', random_state=42)\n",
    "svm_classifier.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = svm_classifier.predict(X_test_scaled)\n",
    "\n",
    "print(\"SVM (Linear Kernel) Classification Report\")\n",
    "print(classification_report(y_test, y_pred_svm))\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T10:06:33.966207Z",
     "iopub.execute_input": "2024-07-25T10:06:33.966643Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# 随机森林\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.fit(X_train_scaled, y_train)\n",
    "y_pred_rf = rf_classifier.predict(X_test_scaled)\n",
    "\n",
    "print(\"Random Forest Classification Report\")\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T10:00:02.439019Z",
     "iopub.execute_input": "2024-07-25T10:00:02.439395Z",
     "iopub.status.idle": "2024-07-25T10:00:53.60281Z",
     "shell.execute_reply.started": "2024-07-25T10:00:02.439367Z",
     "shell.execute_reply": "2024-07-25T10:00:53.601696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# 可视化混淆矩阵\n",
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=['Excellent', 'Good', 'Light Pollution', 'Moderate Pollution', 'Heavy Pollution', 'Severe Pollution'])\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Excellent', 'Good', 'Light Pollution', 'Moderate Pollution', 'Heavy Pollution', 'Severe Pollution'], yticklabels=['Excellent', 'Good', 'Light Pollution', 'Moderate Pollution', 'Heavy Pollution', 'Severe Pollution'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred_dt, \"Decision Tree Confusion Matrix\")\n",
    "# plot_confusion_matrix(y_test, y_pred_svm, \"SVM Confusion Matrix\")\n",
    "plot_confusion_matrix(y_test, y_pred_rf, \"Random Forest Confusion Matrix\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-25T10:01:00.341293Z",
     "iopub.execute_input": "2024-07-25T10:01:00.34169Z",
     "iopub.status.idle": "2024-07-25T10:01:01.617017Z",
     "shell.execute_reply.started": "2024-07-25T10:01:00.34166Z",
     "shell.execute_reply": "2024-07-25T10:01:01.616028Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
